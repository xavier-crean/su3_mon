{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import matplotlib\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ls = np.array(\n",
    "    [\n",
    "        [8, 32, 32, 32],\n",
    "        [8, 40, 40, 40],\n",
    "        [8, 48, 48, 48],\n",
    "        [8, 56, 56, 56],\n",
    "        [8, 64, 64, 64],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Set plotting options\n",
    "plt.rcParams[\"font.size\"] = \"10\"\n",
    "plt.rcParams[\"figure.figsize\"] = (5, 5)\n",
    "cmap = matplotlib.colormaps[\"viridis\"]\n",
    "cm_subsection = np.array(np.linspace(0, 200, len(Ls)), dtype=int)\n",
    "colors = [cmap.colors[x] for x in cm_subsection]\n",
    "fmt = [\"_\", \"*\", \"p\", \"D\", \"^\", \"s\", \"o\"]\n",
    "markersize = 12\n",
    "capsize = 3\n",
    "\n",
    "# Number of lattice sites = volume\n",
    "vol_tot = np.prod(Ls, axis=1, dtype=np.float64)\n",
    "\n",
    "# spatial volume\n",
    "vol = np.prod(Ls[:,1:], axis=1, dtype=np.float64)\n",
    "\n",
    "# Number of Samples\n",
    "N = 400\n",
    "\n",
    "bootstrap_seed = 9999\n",
    "\n",
    "# Reweighting parameters: Set number of bootstraps, reweighting convergence tolerance, reweighting partition and dati header\n",
    "N_bootstraps = 2000\n",
    "tol = 1e-14\n",
    "n_part = 50\n",
    "dati_head = 5 \n",
    "\n",
    "# Literature critical beta 6.0625(18)\n",
    "literature_beta = 6.0625\n",
    "lit_beta_min = 6.0607\n",
    "lit_beta_max = 6.0643"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"../reports/Nt={Ls[0,0]}/figures/\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reweight observables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Uncomment following two lines for linux compilation\n",
    "# %%bash\n",
    "# python3 setup.py build_ext --inplace\n",
    "\n",
    "###Uncomment following two lines for cmd compilation\n",
    "# !python setup.py build_ext --inplace -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import reweight\n",
    "print(reweight.__file__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"../cached_data/\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reweight $\\rho_0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reweighting beta values for each lattice size\n",
    "## to stabilise numerics for smaller lattices sizes\n",
    "betas_k = [\n",
    "    np.array(\n",
    "        [\n",
    "            6.0545, 6.0565, 6.0585, 6.0625, 6.0645, 6.0665, 6.0700\n",
    "        ]\n",
    "    ),\n",
    "    np.array(\n",
    "        [\n",
    "           6.0585, 6.0600, 6.0625, 6.0645, 6.0665, 6.0700,\n",
    "        ]\n",
    "    ),\n",
    "    np.array(\n",
    "        [\n",
    "            6.0550, 6.0585, 6.06, 6.0625, 6.0645, 6.0665,\n",
    "        ]\n",
    "    ),\n",
    "    np.array(\n",
    "        [\n",
    "            6.061, 6.062, 6.0625, 6.063, 6.0640,\n",
    "        ]\n",
    "    ),\n",
    "    np.array(\n",
    "        [\n",
    "            6.0615, 6.0620, 6.0623, 6.0625, 6.0628, 6.0630, 6.0640,\n",
    "        ]\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error bars are computed via the bootstrapping method\n",
    "\n",
    "# Generate bootstraps\n",
    "np.random.seed(bootstrap_seed)\n",
    "bootstraps = [\n",
    "    [\n",
    "        [\n",
    "            np.sort(np.random.choice(range(N), size=N, replace=True))\n",
    "            for i in range(N_bootstraps)\n",
    "        ]\n",
    "        for b in range(len(betas_k[l]))\n",
    "    ]\n",
    "    for l in range(len(Ls))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = []\n",
    "for l in range(len(Ls)):\n",
    "    h_l = []\n",
    "    for b in betas_k[l]:\n",
    "        # Load Betti number HDF5 file if exists (else compute and save)\n",
    "        filename = f\"../data/observables/betti/{Ls[l][0]}.{Ls[l][1]}.{Ls[l][2]}.{Ls[l][3]}/betti_NtNsNsNs={Ls[l][0]}{Ls[l][1]}{Ls[l][2]}{Ls[l][3]}_b={b:.4f}.csv\"\n",
    "        # print(filename)\n",
    "        if os.path.exists(filename):\n",
    "            h_b = np.loadtxt(filename, delimiter=\",\", dtype=int)\n",
    "        h_l.append(h_b)\n",
    "    h.append(h_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip header line + start at sweep 10_000\n",
    "E_l = []\n",
    "for l in range(len(Ls)):\n",
    "    print(Ls[l])\n",
    "    E_b = []\n",
    "    for b in range(len(betas_k[l])):\n",
    "        # print(betas_k[l][b])\n",
    "        averages = []\n",
    "        with open(\n",
    "            f\"../data/observables/dati/{Ls[l][0]}.{Ls[l][1]}.{Ls[l][2]}.{Ls[l][3]}/dati.dat{betas_k[l][b]:.4f}\",\n",
    "            \"r\",\n",
    "        ) as file:\n",
    "            for line in file:\n",
    "                elements = line.split()\n",
    "                averages.append(\n",
    "                    1.0 - (0.5 * (np.float64(elements[0]) + np.float64(elements[1])))\n",
    "                )\n",
    "        total_action = 6.0 * vol_tot[l] * np.array(averages, dtype=np.float64)\n",
    "        E_b.append(total_action[dati_head : (dati_head + N)])\n",
    "    E_l.append(E_b)\n",
    "E = E_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute delta_S for use in multiple histogram reweighting\n",
    "E_bar = [np.mean(E[l], axis=0) for l in range(len(Ls))]\n",
    "delta_E = [E[l] - E_bar[l][np.newaxis, :] for l in range(len(Ls))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate b1 bootstrap distribution\n",
    "b0_bootstrap = [\n",
    "    [\n",
    "        np.array([h[l][b][bootstraps[l][b][i]][:, 0] for b in range(len(betas_k[l]))])\n",
    "        for l in range(len(Ls))\n",
    "    ]\n",
    "    for i in range(N_bootstraps)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whole range of betas\n",
    "rw_betas_linear = [\n",
    "    np.linspace(betas_k[l][0], betas_k[l][-1], n_part, dtype=np.float64) for l in range(len(Ls))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rw_means_linear = []\n",
    "rw_vars_linear = []\n",
    "rw_means_linear_err = []\n",
    "rw_vars_linear_err = []\n",
    "pseudo_bcs_b0_bs = []\n",
    "for l in range(len(Ls)):\n",
    "    print(l)\n",
    "    filename = f\"../cached_data/cache_rw_{np.min(rw_betas_linear[l]):.4f}-{np.max(rw_betas_linear[l]):.4f}_NtNsNsNs={Ls[l][0]}{Ls[l][1]}{Ls[l][2]}{Ls[l][3]}_b0.h5\"\n",
    "    if os.path.exists(filename):\n",
    "        with h5py.File(filename, \"r\") as hf:\n",
    "            rw_means_linear.append(hf[\"rw_means_linear\"][()])\n",
    "            rw_means_linear_err.append(hf[\"rw_means_linear_err\"][()])\n",
    "            rw_vars_linear.append(hf[\"rw_vars_linear\"][()])\n",
    "            rw_vars_linear_err.append(hf[\"rw_vars_linear_err\"][()])\n",
    "            pseudo_bcs_b0_bs.append(hf[\"pseudo_bc_bs\"][()])\n",
    "    else:\n",
    "        temp = np.array(h[l])\n",
    "        rw_means_linear_temp, rw_means_linear_temp_err, rw_vars_linear_temp, rw_vars_linear_temp_err, pseudo_bc_temp_bs = reweight.reweight(\n",
    "            temp[:, :, 0], delta_E[l], betas_k[l], rw_betas_linear[l], N_bootstraps=N_bootstraps, tol=tol\n",
    "        )\n",
    "        with h5py.File(filename, \"w\") as hf:\n",
    "            hf.create_dataset(\"rw_means_linear\", data=rw_means_linear_temp)\n",
    "            hf.create_dataset(\"rw_means_linear_err\", data=rw_means_linear_temp_err)\n",
    "            hf.create_dataset(\"rw_vars_linear\", data=rw_vars_linear_temp)\n",
    "            hf.create_dataset(\"rw_vars_linear_err\", data=rw_vars_linear_temp_err)\n",
    "            hf.create_dataset(\"pseudo_bc_bs\", data=pseudo_bc_temp_bs)\n",
    "        rw_means_linear.append(rw_means_linear_temp)\n",
    "        rw_vars_linear.append(rw_vars_linear_temp)\n",
    "        rw_means_linear_err.append(rw_means_linear_temp_err)\n",
    "        rw_vars_linear_err.append(rw_vars_linear_temp_err)\n",
    "        pseudo_bcs_b0_bs.append(pseudo_bc_temp_bs)\n",
    "rw_means_linear_b0 = np.array(rw_means_linear)\n",
    "rw_vars_linear_b0 = np.array(rw_vars_linear)\n",
    "rw_means_linear_err_b0 = np.array(rw_means_linear_err)\n",
    "rw_vars_linear_err_b0 = np.array(rw_vars_linear_err)\n",
    "pseudo_bcs_b0_bs = np.array(pseudo_bcs_b0_bs)\n",
    "rw_means_linear_b0.shape, rw_means_linear_err_b0.shape, rw_vars_linear_b0.shape, rw_vars_linear_err_b0.shape, pseudo_bcs_b0_bs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yerr_b0 = np.std(pseudo_bcs_b0_bs,axis=1)\n",
    "pseudo_bcs_b0 = np.mean(pseudo_bcs_b0_bs,axis=1)\n",
    "yerr_b0, pseudo_bcs_b0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 1/vol\n",
    "y = pseudo_bcs_b0\n",
    "plt.scatter(x, y, s=10)\n",
    "plt.errorbar(x, y, yerr=yerr_b0, fmt='.',capsize=3)\n",
    "plt.axhspan(lit_beta_min, lit_beta_max, color=\"purple\", alpha=0.15)\n",
    "plt.axhline(literature_beta, color=\"purple\", alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reweight $\\rho_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reweighting beta values for each lattice size\n",
    "## to stabilise numerics for smaller lattices sizes\n",
    "betas_k = [\n",
    "    np.array(\n",
    "        [\n",
    "            6.0565, 6.0585, 6.0625, 6.0645, 6.0665,\n",
    "        ]\n",
    "    ),\n",
    "    np.array(\n",
    "        [\n",
    "            6.055, 6.0585, 6.06, 6.0625, 6.0645\n",
    "        ]\n",
    "    ),\n",
    "    np.array(\n",
    "        [\n",
    "            6.06, 6.0625, 6.0645,\n",
    "        ]\n",
    "    ),\n",
    "    np.array(\n",
    "        [\n",
    "            6.062, 6.0625, 6.063,\n",
    "        ]\n",
    "    ),\n",
    "    np.array(\n",
    "        [\n",
    "            6.0623, 6.0625, 6.0628,\n",
    "        ]\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error bars are computed via the bootstrapping method\n",
    "\n",
    "# Generate bootstraps\n",
    "np.random.seed(bootstrap_seed)\n",
    "bootstraps = [\n",
    "    [\n",
    "        [\n",
    "            np.sort(np.random.choice(range(N), size=N, replace=True))\n",
    "            for i in range(N_bootstraps)\n",
    "        ]\n",
    "        for b in range(len(betas_k[l]))\n",
    "    ]\n",
    "    for l in range(len(Ls))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = []\n",
    "for l in range(len(Ls)):\n",
    "    h_l = []\n",
    "    for b in betas_k[l]:\n",
    "        # Load Betti number HDF5 file if exists (else compute and save)\n",
    "        filename = f\"../data/observables/betti/{Ls[l][0]}.{Ls[l][1]}.{Ls[l][2]}.{Ls[l][3]}/betti_NtNsNsNs={Ls[l][0]}{Ls[l][1]}{Ls[l][2]}{Ls[l][3]}_b={b:.4f}.csv\"\n",
    "        # print(filename)\n",
    "        if os.path.exists(filename):\n",
    "            h_b = np.loadtxt(filename, delimiter=\",\", dtype=int)\n",
    "        h_l.append(h_b)\n",
    "    h.append(h_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip header line + start at sweep 10_000\n",
    "E_l = []\n",
    "for l in range(len(Ls)):\n",
    "    print(Ls[l])\n",
    "    E_b = []\n",
    "    for b in range(len(betas_k[l])):\n",
    "        # print(betas_k[l][b])\n",
    "        averages = []\n",
    "        with open(\n",
    "            f\"../data/observables/dati/{Ls[l][0]}.{Ls[l][1]}.{Ls[l][2]}.{Ls[l][3]}/dati.dat{betas_k[l][b]:.4f}\",\n",
    "            \"r\",\n",
    "        ) as file:\n",
    "            for line in file:\n",
    "                elements = line.split()\n",
    "                averages.append(\n",
    "                    1.0 - (0.5 * (np.float64(elements[0]) + np.float64(elements[1])))\n",
    "                )\n",
    "        total_action = 6.0 * vol_tot[l] * np.array(averages, dtype=np.float64)\n",
    "        E_b.append(total_action[dati_head : (dati_head + N)])\n",
    "    E_l.append(E_b)\n",
    "E = E_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute delta_S for use in multiple histogram reweighting\n",
    "E_bar = [np.mean(E[l], axis=0) for l in range(len(Ls))]\n",
    "delta_E = [E[l] - E_bar[l][np.newaxis, :] for l in range(len(Ls))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate b1 bootstrap distribution\n",
    "b1_bootstrap = [\n",
    "    [\n",
    "        np.array([h[l][b][bootstraps[l][b][i]][:, 1] for b in range(len(betas_k[l]))])\n",
    "        for l in range(len(Ls))\n",
    "    ]\n",
    "    for i in range(N_bootstraps)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whole range of betas\n",
    "rw_betas_linear = [\n",
    "    np.linspace(betas_k[l][0], betas_k[l][-1], n_part, dtype=np.float64) for l in range(len(Ls))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rw_means_linear = []\n",
    "rw_vars_linear = []\n",
    "rw_means_linear_err = []\n",
    "rw_vars_linear_err = []\n",
    "pseudo_bcs_b1_bs = []\n",
    "for l in range(len(Ls)):\n",
    "    print(l)\n",
    "    filename = f\"../cached_data/cache_rw_{np.min(rw_betas_linear[l]):.4f}-{np.max(rw_betas_linear[l]):.4f}_NtNsNsNs={Ls[l][0]}{Ls[l][1]}{Ls[l][2]}{Ls[l][3]}_b1.h5\"\n",
    "    if os.path.exists(filename):\n",
    "        with h5py.File(filename, \"r\") as hf:\n",
    "            rw_means_linear.append(hf[\"rw_means_linear\"][()])\n",
    "            rw_means_linear_err.append(hf[\"rw_means_linear_err\"][()])\n",
    "            rw_vars_linear.append(hf[\"rw_vars_linear\"][()])\n",
    "            rw_vars_linear_err.append(hf[\"rw_vars_linear_err\"][()])\n",
    "            pseudo_bcs_b1_bs.append(hf[\"pseudo_bc_bs\"][()])\n",
    "    else:\n",
    "        temp = np.array(h[l])\n",
    "        rw_means_linear_temp, rw_means_linear_temp_err, rw_vars_linear_temp, rw_vars_linear_temp_err, pseudo_bc_temp_bs = reweight.reweight(\n",
    "            temp[:, :, 1], delta_E[l], betas_k[l], rw_betas_linear[l], N_bootstraps=N_bootstraps, tol=tol\n",
    "        )\n",
    "        with h5py.File(filename, \"w\") as hf:\n",
    "            hf.create_dataset(\"rw_means_linear\", data=rw_means_linear_temp)\n",
    "            hf.create_dataset(\"rw_means_linear_err\", data=rw_means_linear_temp_err)\n",
    "            hf.create_dataset(\"rw_vars_linear\", data=rw_vars_linear_temp)\n",
    "            hf.create_dataset(\"rw_vars_linear_err\", data=rw_vars_linear_temp_err)\n",
    "            hf.create_dataset(\"pseudo_bc_bs\", data=pseudo_bc_temp_bs)\n",
    "        rw_means_linear.append(rw_means_linear_temp)\n",
    "        rw_vars_linear.append(rw_vars_linear_temp)\n",
    "        rw_means_linear_err.append(rw_means_linear_temp_err)\n",
    "        rw_vars_linear_err.append(rw_vars_linear_temp_err)\n",
    "        pseudo_bcs_b1_bs.append(pseudo_bc_temp_bs)\n",
    "rw_means_linear_b1 = np.array(rw_means_linear)\n",
    "rw_vars_linear_b1 = np.array(rw_vars_linear)\n",
    "rw_means_linear_err_b1 = np.array(rw_means_linear_err)\n",
    "rw_vars_linear_err_b1 = np.array(rw_vars_linear_err)\n",
    "pseudo_bcs_b1_bs = np.array(pseudo_bcs_b1_bs)\n",
    "rw_means_linear_b1.shape, rw_means_linear_err_b1.shape, rw_vars_linear_b1.shape, rw_vars_linear_err_b1.shape, pseudo_bcs_b1_bs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yerr_b1 = np.std(pseudo_bcs_b1_bs,axis=1)\n",
    "pseudo_bcs_b1 = np.mean(pseudo_bcs_b1_bs,axis=1)\n",
    "yerr_b1, pseudo_bcs_b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 1/vol\n",
    "y = pseudo_bcs_b1\n",
    "plt.scatter(x, y, s=10)\n",
    "plt.errorbar(x, y, yerr=yerr_b1, fmt='.',capsize=3)\n",
    "plt.axhspan(lit_beta_min, lit_beta_max, color=\"purple\", alpha=0.15)\n",
    "plt.axhline(literature_beta, color=\"purple\", alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reweight $\\lambda$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reweighting beta values for each lattice size\n",
    "## to stabilise numerics for smaller lattices sizes\n",
    "betas_k = [\n",
    "    np.array(\n",
    "        [\n",
    "            6.0545, 6.0565, 6.0585, 6.0625, 6.0645, 6.0665, 6.0700, 6.0750, 6.0800,\n",
    "        ]\n",
    "    ),\n",
    "    np.array(\n",
    "        [\n",
    "           6.0550, 6.0585, 6.0600, 6.0625, 6.0645, 6.0665, 6.0700,\n",
    "        ]\n",
    "    ),\n",
    "    np.array(\n",
    "        [\n",
    "            6.0600, 6.0625, 6.0645, 6.0665,\n",
    "        ]\n",
    "    ),\n",
    "    np.array(\n",
    "        [\n",
    "            6.0610, 6.0620, 6.0625, 6.0630, 6.0640,\n",
    "        ]\n",
    "    ),\n",
    "    np.array(\n",
    "        [\n",
    "            6.0615, 6.0620, 6.0623, 6.0625, 6.0628, 6.0630, 6.0640,\n",
    "        ]\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error bars are computed via the bootstrapping method\n",
    "\n",
    "# Generate bootstraps\n",
    "np.random.seed(bootstrap_seed)\n",
    "bootstraps = [\n",
    "    [\n",
    "        [\n",
    "            np.sort(np.random.choice(range(N), size=N, replace=True))\n",
    "            for i in range(N_bootstraps)\n",
    "        ]\n",
    "        for b in range(len(betas_k[l]))\n",
    "    ]\n",
    "    for l in range(len(Ls))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = []\n",
    "for l in range(len(Ls)):\n",
    "    h_l = []\n",
    "    for b in betas_k[l]:\n",
    "        # Load Betti number HDF5 file if exists (else compute and save)\n",
    "        filename = f\"../data/observables/betti/{Ls[l][0]}.{Ls[l][1]}.{Ls[l][2]}.{Ls[l][3]}/betti_NtNsNsNs={Ls[l][0]}{Ls[l][1]}{Ls[l][2]}{Ls[l][3]}_b={b:.4f}.csv\"\n",
    "        # print(filename)\n",
    "        if os.path.exists(filename):\n",
    "            h_b = np.loadtxt(filename, delimiter=\",\", dtype=int)\n",
    "        h_l.append(h_b)\n",
    "    h.append(h_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip header line + start at sweep 10_000\n",
    "E_l = []\n",
    "for l in range(len(Ls)):\n",
    "    print(Ls[l])\n",
    "    E_b = []\n",
    "    for b in range(len(betas_k[l])):\n",
    "        # print(betas_k[l][b])\n",
    "        averages = []\n",
    "        with open(\n",
    "            f\"../data/observables/dati/{Ls[l][0]}.{Ls[l][1]}.{Ls[l][2]}.{Ls[l][3]}/dati.dat{betas_k[l][b]:.4f}\",\n",
    "            \"r\",\n",
    "        ) as file:\n",
    "            for line in file:\n",
    "                elements = line.split()\n",
    "                averages.append(\n",
    "                    1.0 - (0.5 * (np.float64(elements[0]) + np.float64(elements[1])))\n",
    "                )\n",
    "        total_action = 6.0 * vol_tot[l] * np.array(averages, dtype=np.float64)\n",
    "        E_b.append(total_action[dati_head : (dati_head + N)])\n",
    "    E_l.append(E_b)\n",
    "E = E_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute delta_S for use in multiple histogram reweighting\n",
    "E_bar = [np.mean(E[l], axis=0) for l in range(len(Ls))]\n",
    "delta_E = [E[l] - E_bar[l][np.newaxis, :] for l in range(len(Ls))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate b1 bootstrap distribution\n",
    "ratio01_bootstrap = [\n",
    "    [\n",
    "        np.array([(h[l][b][bootstraps[l][b][i]][:, 0] / h[l][b][bootstraps[l][b][i]][:, 1]) for b in range(len(betas_k[l]))])\n",
    "        for l in range(len(Ls))\n",
    "    ]\n",
    "    for i in range(N_bootstraps)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whole range of betas\n",
    "rw_betas_linear = [\n",
    "    np.linspace(betas_k[l][0], betas_k[l][-1], n_part, dtype=np.float64) for l in range(len(Ls))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rw_means_linear = []\n",
    "rw_vars_linear = []\n",
    "rw_means_linear_err = []\n",
    "rw_vars_linear_err = []\n",
    "pseudo_bcs_r01_bs = []\n",
    "for l in range(len(Ls)):\n",
    "    print(l)\n",
    "    filename = f\"../cached_data/cache_rw_{np.min(rw_betas_linear[l]):.4f}-{np.max(rw_betas_linear[l]):.4f}_NtNsNsNs={Ls[l][0]}{Ls[l][1]}{Ls[l][2]}{Ls[l][3]}_r01.h5\"\n",
    "    if os.path.exists(filename):\n",
    "        with h5py.File(filename, \"r\") as hf:\n",
    "            rw_means_linear.append(hf[\"rw_means_linear\"][()])\n",
    "            rw_means_linear_err.append(hf[\"rw_means_linear_err\"][()])\n",
    "            rw_vars_linear.append(hf[\"rw_vars_linear\"][()])\n",
    "            rw_vars_linear_err.append(hf[\"rw_vars_linear_err\"][()])\n",
    "            pseudo_bcs_r01_bs.append(hf[\"pseudo_bc_bs\"][()])\n",
    "    else:\n",
    "        temp = np.array(h[l])\n",
    "        rw_means_linear_temp, rw_means_linear_temp_err, rw_vars_linear_temp, rw_vars_linear_temp_err, pseudo_bc_temp_bs = reweight.reweight(\n",
    "            (temp[:, :, 0] / temp[:, :, 1]), delta_E[l], betas_k[l], rw_betas_linear[l], N_bootstraps=N_bootstraps, tol=tol\n",
    "        )\n",
    "        with h5py.File(filename, \"w\") as hf:\n",
    "            hf.create_dataset(\"rw_means_linear\", data=rw_means_linear_temp)\n",
    "            hf.create_dataset(\"rw_means_linear_err\", data=rw_means_linear_temp_err)\n",
    "            hf.create_dataset(\"rw_vars_linear\", data=rw_vars_linear_temp)\n",
    "            hf.create_dataset(\"rw_vars_linear_err\", data=rw_vars_linear_temp_err)\n",
    "            hf.create_dataset(\"pseudo_bc_bs\", data=pseudo_bc_temp_bs)\n",
    "        rw_means_linear.append(rw_means_linear_temp)\n",
    "        rw_vars_linear.append(rw_vars_linear_temp)\n",
    "        rw_means_linear_err.append(rw_means_linear_temp_err)\n",
    "        rw_vars_linear_err.append(rw_vars_linear_temp_err)\n",
    "        pseudo_bcs_r01_bs.append(pseudo_bc_temp_bs)\n",
    "rw_means_linear_r01 = np.array(rw_means_linear)\n",
    "rw_vars_linear_r01 = np.array(rw_vars_linear)\n",
    "rw_means_linear_err_r01 = np.array(rw_means_linear_err)\n",
    "rw_vars_linear_err_r01 = np.array(rw_vars_linear_err)\n",
    "pseudo_bcs_r01_bs = np.array(pseudo_bcs_r01_bs)\n",
    "rw_means_linear_r01.shape, rw_means_linear_err_r01.shape, rw_vars_linear_r01.shape, rw_vars_linear_err_r01.shape, pseudo_bcs_r01_bs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yerr_r01 = np.std(pseudo_bcs_r01_bs,axis=1)\n",
    "pseudo_bcs_r01 = np.mean(pseudo_bcs_r01_bs,axis=1)\n",
    "yerr_r01, pseudo_bcs_r01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 1/vol\n",
    "y = pseudo_bcs_r01\n",
    "plt.scatter(x, y, s=10)\n",
    "plt.errorbar(x, y, yerr=yerr_r01, fmt='.',capsize=3)\n",
    "plt.axhspan(lit_beta_min, lit_beta_max, color=\"purple\", alpha=0.15)\n",
    "plt.axhline(literature_beta, color=\"purple\", alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finite-size scaling analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import brentq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_NIST(m, u):\n",
    "    \"\"\"Formats measurement and standard error in NIST:\n",
    "    https://physics.nist.gov/cgi-bin/cuu/Info/Constants/definitions.html\n",
    "    \"\"\"\n",
    "    m = float(m)\n",
    "    u = float(u)\n",
    "    u_order = int(np.floor(np.log10(u)))\n",
    "    round_m = \"{:.{}f}\".format(m, -1 * u_order)\n",
    "    round_u = str(int(np.round(u * 10 ** (-u_order))))\n",
    "    return round_m + \"(\" + round_u + \")\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fss(type, Ls_for_reg, deg, y, y_bs):\n",
    "    \"\"\"Function to output range, k_max, reduced_chi2, AIC, beta_c, beta_c_err.\n",
    "       Returns None if an unacceptable fit (with internal extremum) is detected.\n",
    "    \"\"\"\n",
    "    # Perform polynomial regression\n",
    "    poly_features = PolynomialFeatures(degree=deg, include_bias=False)\n",
    "    X_poly = poly_features.fit_transform(x.reshape(-1, 1))\n",
    "    lin_reg = LinearRegression().fit(X_poly[Ls_for_reg], y[Ls_for_reg])\n",
    "\n",
    "    if deg >= 2:\n",
    "        # Descending order: [a_d, ..., a_1, a_0]\n",
    "        p_coefs_desc = list(lin_reg.coef_[::-1]) + [lin_reg.intercept_]\n",
    "        der_coefs = np.polyder(p_coefs_desc)\n",
    "        roots = np.roots(der_coefs)\n",
    "        real_roots = roots[np.isreal(roots)].real\n",
    "        x_min = 0.0\n",
    "        x_max = x.max()\n",
    "        # Check if any real root is strictly inside the data range\n",
    "        if np.any((real_roots > x_min) & (real_roots < x_max)):\n",
    "            print(\"Rejecting fit; found internal extremum at\", real_roots)\n",
    "            return None\n",
    "\n",
    "    # Compute intercept error via bootstrap\n",
    "    intercepts = [\n",
    "        LinearRegression().fit(X_poly[Ls_for_reg], y_bs[i][Ls_for_reg]).intercept_\n",
    "        for i in range(N_bootstraps)\n",
    "    ]\n",
    "    intercept_err = np.sqrt(np.var(intercepts))\n",
    "\n",
    "    # Compute reduced chi-squared\n",
    "    f_obs = y[Ls_for_reg]\n",
    "    f_exp = lin_reg.predict(X_poly[Ls_for_reg])\n",
    "    var = np.var(y_bs[:, Ls_for_reg], axis=0)\n",
    "    chi2 = np.sum((f_obs - f_exp) ** 2 / var)\n",
    "\n",
    "    # Calculate AIC\n",
    "    n_par = deg + 1\n",
    "    n_data = len(Ls_for_reg)\n",
    "    aic_unnorm = np.exp(-0.5 * (chi2 + 2 * n_par - n_data))\n",
    "\n",
    "    return (\n",
    "        type,\n",
    "        str(Ls[Ls_for_reg][0]),\n",
    "        str(Ls[Ls_for_reg][-1]),\n",
    "        deg,\n",
    "        chi2,\n",
    "        aic_unnorm,\n",
    "        lin_reg.intercept_,\n",
    "        list(lin_reg.coef_),\n",
    "        intercept_err,\n",
    "        f\"{chi2:.3f}\",\n",
    "        f\"{aic_unnorm:.3f}\",\n",
    "        format_NIST(lin_reg.intercept_, intercept_err),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mixture CDF: sum_i w_i * N(x|mu_i, sigma_i)\n",
    "def mixture_cdf(x, weights, means, sigmas):\n",
    "    return np.sum(weights * norm.cdf(x, loc=means, scale=sigmas))\n",
    "\n",
    "# Function to find x such that the CDF equals a target value.\n",
    "def find_quantile(target, weights, means, sigmas, x_min, x_max):\n",
    "    func = lambda x: mixture_cdf(x, weights, means, sigmas) - target\n",
    "    return brentq(func, x_min, x_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_asymmetric_error(lower, central, upper):\n",
    "    \"\"\"\n",
    "    Given lower (X_16), central (X_50), and upper (X_84) values as floats,\n",
    "    compute asymmetric uncertainties, round them to appropriate significant figures,\n",
    "    and return a LaTeX-formatted string.\n",
    "    \"\"\"\n",
    "    # Compute the deltas\n",
    "    delta_minus = central - lower\n",
    "    delta_plus = upper - central\n",
    "\n",
    "    def round_uncertainty(d):\n",
    "        \"\"\"Round uncertainty to 1 significant figure, or 2 if first digit is 1.\"\"\"\n",
    "        if d == 0:\n",
    "            return 0.0, 0\n",
    "        exp = np.floor(np.log10(d))\n",
    "        first_digit = d / (10 ** exp)\n",
    "        sig_figs = 2 if 1 <= first_digit < 2 else 1\n",
    "        # Number of decimal places for round()\n",
    "        decimals = max(0, int(-exp + (sig_figs - 1)))\n",
    "        return round(d, decimals), decimals\n",
    "\n",
    "    rm, dec_m = round_uncertainty(delta_minus)\n",
    "    rp, dec_p = round_uncertainty(delta_plus)\n",
    "    # Decimal places for central value\n",
    "    dec_c = max(dec_m, dec_p)\n",
    "    central_rounded = round(central, dec_c)\n",
    "\n",
    "    # Build the LaTeX string\n",
    "    fmt = f\"${central_rounded:.{dec_c}f}\" \\\n",
    "          f\"^{{+{rp:.{dec_c}f}}}_{{-{rm:.{dec_c}f}}}$\"\n",
    "          \n",
    "    return fmt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\rho_0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range of Ls values to be used in FSS\n",
    "range_ = [[2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 3, 4]]\n",
    "\n",
    "# When computing results, filter out any models where fss returns None\n",
    "results = []\n",
    "for type, ps_bc, ps_bc_var in [(\"rho_b0\", pseudo_bcs_b0, pseudo_bcs_b0_bs.T)]:\n",
    "    for i in range(len(range_)):\n",
    "        for d in range(1, len(range_[i]) - 1):\n",
    "            model = fss(type, range_[i], d, ps_bc, ps_bc_var)\n",
    "            if model is not None:\n",
    "                results.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.DataFrame(results)\n",
    "df.columns = [\n",
    "    \"observable\",\n",
    "    \"range_min\",\n",
    "    \"range_max\",\n",
    "    \"k_max\",\n",
    "    \"chi2\",\n",
    "    \"aic_unnorm\",\n",
    "    \"beta_c\",\n",
    "    \"coefficients\",\n",
    "    \"beta_c_err\",\n",
    "    \"str_chi2\",\n",
    "    \"str_aic\",\n",
    "    \"beta_c_NIST\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract AIC, intercepts, and errors\n",
    "weight_unnorms_b0 = np.array([model[5] for model in results])  # Assuming AIC is the 6th element\n",
    "beta_c_values_b0 = np.array([model[6] for model in results])  # Intercept\n",
    "beta_c_errs_b0 = np.array([model[8] for model in results])  # Intercept error\n",
    "\n",
    "# Compute model weights (pr(M|D))\n",
    "weights_b0 = weight_unnorms_b0 / np.sum(weight_unnorms_b0)\n",
    "weights_b0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a search interval that should comfortably include the distribution.\n",
    "x_min_search = np.min(beta_c_values_b0) - 5 * np.max(beta_c_errs_b0)\n",
    "x_max_search = np.max(beta_c_values_b0) + 5 * np.max(beta_c_errs_b0)\n",
    "\n",
    "# Compute the quantiles for the median and the 68% interval.\n",
    "lower_val = find_quantile(0.16, weights_b0, beta_c_values_b0, beta_c_errs_b0, x_min_search, x_max_search)\n",
    "median_val = find_quantile(0.5, weights_b0, beta_c_values_b0, beta_c_errs_b0, x_min_search, x_max_search)\n",
    "upper_val = find_quantile(0.84, weights_b0, beta_c_values_b0, beta_c_errs_b0, x_min_search, x_max_search)\n",
    "\n",
    "# Compute asymmetric errors (distance from the median to the lower and upper bounds)\n",
    "error_lower = median_val - lower_val\n",
    "error_upper = upper_val - median_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_lower, median_val, error_upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_str = format_asymmetric_error(lower_val, median_val, upper_val)\n",
    "print(latex_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.axhline(literature_beta, color=\"purple\", linestyle=(0, (3, 1, 1, 1)), alpha=0.5, label=r\"Liter. $\\beta_c (\\infty, N_t =$\"+f\"{Ls[0,0]}\"+r\"$)$ estimate\")\n",
    "plt.axhspan(lit_beta_min, lit_beta_max, color=\"purple\", alpha=0.1, label=r\"Liter. $\\beta_c (\\infty, N_t =$\"+f\"{Ls[0,0]}\"+r\"$)$ std. error\")\n",
    "\n",
    "\n",
    "# Plot each polynomial fit\n",
    "for j,row in enumerate(df.rows(named=True)):\n",
    "    intercept = row[\"beta_c\"]\n",
    "    coefficients = row[\"coefficients\"]\n",
    "    deg = row[\"k_max\"]\n",
    "    \n",
    "    # Generate smooth x values for plotting the curve\n",
    "    x_plot = np.linspace(0, max(x)*1.1, 300)\n",
    "    \n",
    "    # Manually compute polynomial: intercept + c1*x + c2*x² + ...\n",
    "    y_plot = intercept + sum(\n",
    "        coef * (x_plot ** (i + 1)) \n",
    "        for i, coef in enumerate(coefficients)\n",
    "    )\n",
    "    \n",
    "    rangemin = list(map(int, row['range_min'].strip(\"[]\").split()))[1]\n",
    "    rangemax = list(map(int, row['range_max'].strip(\"[]\").split()))[1]\n",
    "    Ns_range = np.arange(rangemin, rangemax+1, Ls[0,0])\n",
    "    Ns_range_str = \", \".join(str(l) for l in Ns_range)\n",
    "    \n",
    "    # Label the fit\n",
    "    label = f\"Polyn. degree: {deg}\\n$N_s$ values: {Ns_range_str}\"\n",
    "    plt.plot(x_plot, y_plot, \"--\", label=label, alpha=0.7)\n",
    "    \n",
    "    \n",
    "x = 1/vol\n",
    "y = pseudo_bcs_b0\n",
    "plt.errorbar(x, y, yerr=yerr_b0, fmt=\"^\", label=r\"$\\beta_c ( N_s, N_t =$\"+f\"{Ls[0,0]}\"+r\"$)$\", capsize=7, color=\"blue\", markersize=7)\n",
    "plt.errorbar(0, median_val, yerr=[[error_lower], [error_upper]], fmt=\".\", label=r\"$\\beta_c (\\infty, N_t =$\"+f\"{Ls[0,0]}\"+r\"$)$ estimate\", capsize=7, color=\"red\", markersize=7)\n",
    "\n",
    "plt.xlabel(r\"$N_{s}^{-3}$\",fontsize=30)\n",
    "plt.ylabel(r\"$\\beta$\", fontsize=30,rotation=0,labelpad=15)\n",
    "plt.ylim([6.059, 6.075])\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.ticklabel_format(axis='x', style='sci', scilimits=(0, 0))\n",
    "ax.xaxis.get_offset_text().set_fontsize(20)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "# split handles/labels into \"fits\" vs \"others\"\n",
    "is_fit = [lab.startswith(\"Polyn.\") for lab in labels]\n",
    "fit_handles  = [h for h,f in zip(handles,is_fit) if f]\n",
    "fit_labels   = [l for l,f in zip(labels,is_fit) if f]\n",
    "other_handles = [h for h,f in zip(handles,is_fit) if not f]\n",
    "other_labels  = [l for l,f in zip(labels,is_fit) if not f]\n",
    "\n",
    "leg1 = ax.legend(\n",
    "    other_handles, other_labels,\n",
    "    loc='upper left',\n",
    "    frameon=True,\n",
    "    ncol=2,\n",
    "    fontsize=13\n",
    ")\n",
    "leg2 = ax.legend(\n",
    "    fit_handles, fit_labels,\n",
    "    loc='upper left',\n",
    "    bbox_to_anchor=(0.0, 0.85),\n",
    "    frameon=True,\n",
    "    title=\"Fits\",\n",
    "    ncol=2,\n",
    "    fontsize=12\n",
    ")\n",
    "ax.add_artist(leg1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"../reports/Nt={Ls[0,0]}/figures/fits_rho0.pdf\", format=\"pdf\",bbox_inches='tight',pad_inches=0.025)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\rho_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range of Ls values to be used in FSS\n",
    "range_ = [[2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 3, 4]]\n",
    "\n",
    "# When computing results, filter out any models where fss returns None\n",
    "results = []\n",
    "for type, ps_bc, ps_bc_var in [(\"rho_b1\", pseudo_bcs_b1, pseudo_bcs_b1_bs.T)]:\n",
    "    for i in range(len(range_)):\n",
    "        for d in range(1, len(range_[i]) - 1):\n",
    "            model = fss(type, range_[i], d, ps_bc, ps_bc_var)\n",
    "            if model is not None:\n",
    "                results.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.DataFrame(results)\n",
    "df.columns = [\n",
    "    \"observable\",\n",
    "    \"range_min\",\n",
    "    \"range_max\",\n",
    "    \"k_max\",\n",
    "    \"chi2\",\n",
    "    \"aic_unnorm\",\n",
    "    \"beta_c\",\n",
    "    \"coefficients\",\n",
    "    \"beta_c_err\",\n",
    "    \"str_chi2\",\n",
    "    \"str_aic\",\n",
    "    \"beta_c_NIST\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract AIC, intercepts, and errors\n",
    "weight_unnorms_b1 = np.array([model[5] for model in results])  # Assuming AIC is the 6th element\n",
    "beta_c_values_b1 = np.array([model[6] for model in results])  # Intercept\n",
    "beta_c_errs_b1 = np.array([model[8] for model in results])  # Intercept error\n",
    "\n",
    "# Compute model weights (pr(M|D))\n",
    "weights_b1 = weight_unnorms_b1 / np.sum(weight_unnorms_b1)\n",
    "weights_b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a search interval that should comfortably include the distribution.\n",
    "x_min_search = np.min(beta_c_values_b1) - 5 * np.max(beta_c_errs_b1)\n",
    "x_max_search = np.max(beta_c_values_b1) + 5 * np.max(beta_c_errs_b1)\n",
    "\n",
    "# Compute the quantiles for the median and the 68% interval.\n",
    "lower_val = find_quantile(0.16, weights_b1, beta_c_values_b1, beta_c_errs_b1, x_min_search, x_max_search)\n",
    "median_val = find_quantile(0.5, weights_b1, beta_c_values_b1, beta_c_errs_b1, x_min_search, x_max_search)\n",
    "upper_val = find_quantile(0.84, weights_b1, beta_c_values_b1, beta_c_errs_b1, x_min_search, x_max_search)\n",
    "\n",
    "# Compute asymmetric errors (distance from the median to the lower and upper bounds)\n",
    "error_lower = median_val - lower_val\n",
    "error_upper = upper_val - median_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_lower, median_val, error_upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_str = format_asymmetric_error(lower_val, median_val, upper_val)\n",
    "print(latex_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.axhline(literature_beta, color=\"purple\", linestyle=(0, (3, 1, 1, 1)), alpha=0.5, label=r\"Liter. $\\beta_c (\\infty, N_t =$\"+f\"{Ls[0,0]}\"+r\"$)$ estimate\")\n",
    "plt.axhspan(lit_beta_min, lit_beta_max, color=\"purple\", alpha=0.1, label=r\"Liter. $\\beta_c (\\infty, N_t =$\"+f\"{Ls[0,0]}\"+r\"$)$ std. error\")\n",
    "\n",
    "\n",
    "# Plot each polynomial fit\n",
    "for j,row in enumerate(df.rows(named=True)):\n",
    "    intercept = row[\"beta_c\"]\n",
    "    coefficients = row[\"coefficients\"]\n",
    "    deg = row[\"k_max\"]\n",
    "    \n",
    "    # Generate smooth x values for plotting the curve\n",
    "    x_plot = np.linspace(0, max(x)*1.1, 300)\n",
    "    \n",
    "    # Manually compute polynomial: intercept + c1*x + c2*x² + ...\n",
    "    y_plot = intercept + sum(\n",
    "        coef * (x_plot ** (i + 1)) \n",
    "        for i, coef in enumerate(coefficients)\n",
    "    )\n",
    "    \n",
    "    rangemin = list(map(int, row['range_min'].strip(\"[]\").split()))[1]\n",
    "    rangemax = list(map(int, row['range_max'].strip(\"[]\").split()))[1]\n",
    "    Ns_range = np.arange(rangemin, rangemax+1, Ls[0,0])\n",
    "    Ns_range_str = \", \".join(str(l) for l in Ns_range)\n",
    "    \n",
    "    # Label the fit\n",
    "    label = f\"Polyn. degree: {deg}\\n$N_s$ values: {Ns_range_str}\"\n",
    "    plt.plot(x_plot, y_plot, \"--\", label=label, alpha=0.7)\n",
    "    \n",
    "    \n",
    "x = 1/vol\n",
    "y = pseudo_bcs_b1\n",
    "plt.errorbar(x, y, yerr=yerr_b1, fmt=\"^\", label=r\"$\\beta_c ( N_s, N_t =$\"+f\"{Ls[0,0]}\"+r\"$)$\", capsize=7, color=\"blue\", markersize=7)\n",
    "plt.errorbar(0, median_val, yerr=[[error_lower], [error_upper]], fmt=\".\", label=r\"$\\beta_c (\\infty, N_t =$\"+f\"{Ls[0,0]}\"+r\"$)$ estimate\", capsize=7, color=\"red\", markersize=7)\n",
    "\n",
    "plt.xlabel(r\"$N_{s}^{-3}$\",fontsize=30)\n",
    "plt.ylabel(r\"$\\beta$\",fontsize=30,rotation=0,labelpad=15)\n",
    "plt.ylim([6.0590, 6.0690])\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.ticklabel_format(axis='x', style='sci', scilimits=(0, 0))\n",
    "ax.xaxis.get_offset_text().set_fontsize(20)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "# split handles/labels into \"fits\" vs \"others\"\n",
    "is_fit = [lab.startswith(\"Polyn.\") for lab in labels]\n",
    "fit_handles  = [h for h,f in zip(handles,is_fit) if f]\n",
    "fit_labels   = [l for l,f in zip(labels,is_fit) if f]\n",
    "other_handles = [h for h,f in zip(handles,is_fit) if not f]\n",
    "other_labels  = [l for l,f in zip(labels,is_fit) if not f]\n",
    "\n",
    "leg1 = ax.legend(\n",
    "    other_handles, other_labels,\n",
    "    loc='upper left',\n",
    "    frameon=True,\n",
    "    ncol=2,\n",
    "    fontsize=13\n",
    ")\n",
    "leg2 = ax.legend(\n",
    "    fit_handles, fit_labels,\n",
    "    loc='upper left',\n",
    "    bbox_to_anchor=(0.0, 0.85),\n",
    "    frameon=True,\n",
    "    title=\"Fits\",\n",
    "    ncol=2,\n",
    "    fontsize=12\n",
    ")\n",
    "ax.add_artist(leg1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"../reports/Nt={Ls[0,0]}/figures/fits_rho1.pdf\",format=\"pdf\",bbox_inches='tight',pad_inches=0.025)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\lambda$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range of Ls values to be used in FSS\n",
    "range_ = [[2, 3, 4], [1, 2, 3, 4], [0, 1, 2, 3, 4]]\n",
    "\n",
    "# When computing results, filter out any models where fss returns None\n",
    "results = []\n",
    "for type, ps_bc, ps_bc_var in [(\"r01\", pseudo_bcs_r01, pseudo_bcs_r01_bs.T)]:\n",
    "    for i in range(len(range_)):\n",
    "        for d in range(1, len(range_[i]) - 1):\n",
    "            model = fss(type, range_[i], d, ps_bc, ps_bc_var)\n",
    "            if model is not None:\n",
    "                results.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.DataFrame(results)\n",
    "df.columns = [\n",
    "    \"observable\",\n",
    "    \"range_min\",\n",
    "    \"range_max\",\n",
    "    \"k_max\",\n",
    "    \"chi2\",\n",
    "    \"aic_unnorm\",\n",
    "    \"beta_c\",\n",
    "    \"coefficients\",\n",
    "    \"beta_c_err\",\n",
    "    \"str_chi2\",\n",
    "    \"str_aic\",\n",
    "    \"beta_c_NIST\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract AIC, intercepts, and errors\n",
    "weight_unnorms_b1 = np.array([model[5] for model in results])  # Assuming AIC is the 6th element\n",
    "beta_c_values_b1 = np.array([model[6] for model in results])  # Intercept\n",
    "beta_c_errs_b1 = np.array([model[8] for model in results])  # Intercept error\n",
    "\n",
    "# Compute model weights (pr(M|D))\n",
    "weights_b1 = weight_unnorms_b1 / np.sum(weight_unnorms_b1)\n",
    "weights_b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a search interval that should comfortably include the distribution.\n",
    "x_min_search = np.min(beta_c_values_b1) - 5 * np.max(beta_c_errs_b1)\n",
    "x_max_search = np.max(beta_c_values_b1) + 5 * np.max(beta_c_errs_b1)\n",
    "\n",
    "# Compute the quantiles for the median and the 68% interval.\n",
    "lower_val = find_quantile(0.16, weights_b1, beta_c_values_b1, beta_c_errs_b1, x_min_search, x_max_search)\n",
    "median_val = find_quantile(0.5, weights_b1, beta_c_values_b1, beta_c_errs_b1, x_min_search, x_max_search)\n",
    "upper_val = find_quantile(0.84, weights_b1, beta_c_values_b1, beta_c_errs_b1, x_min_search, x_max_search)\n",
    "\n",
    "# Compute asymmetric errors (distance from the median to the lower and upper bounds)\n",
    "error_lower = median_val - lower_val\n",
    "error_upper = upper_val - median_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_lower, median_val, error_upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_str = format_asymmetric_error(lower_val, median_val, upper_val)\n",
    "print(latex_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.axhline(literature_beta, color=\"purple\", linestyle=(0, (3, 1, 1, 1)), alpha=0.5, label=r\"Liter. $\\beta_c (\\infty, N_t =$\"+f\"{Ls[0,0]}\"+r\"$)$ estimate\")\n",
    "plt.axhspan(lit_beta_min, lit_beta_max, color=\"purple\", alpha=0.1, label=r\"Liter. $\\beta_c (\\infty, N_t =$\"+f\"{Ls[0,0]}\"+r\"$)$ std. error\")\n",
    "\n",
    "# Plot each polynomial fit\n",
    "for j,row in enumerate(df.rows(named=True)):\n",
    "    intercept = row[\"beta_c\"]\n",
    "    coefficients = row[\"coefficients\"]\n",
    "    deg = row[\"k_max\"]\n",
    "    \n",
    "    # Generate smooth x values for plotting the curve\n",
    "    x_plot = np.linspace(0, max(x)*1.1, 300)\n",
    "    \n",
    "    # Manually compute polynomial: intercept + c1*x + c2*x² + ...\n",
    "    y_plot = intercept + sum(\n",
    "        coef * (x_plot ** (i + 1)) \n",
    "        for i, coef in enumerate(coefficients)\n",
    "    )\n",
    "    \n",
    "    rangemin = list(map(int, row['range_min'].strip(\"[]\").split()))[1]\n",
    "    rangemax = list(map(int, row['range_max'].strip(\"[]\").split()))[1]\n",
    "    Ns_range = np.arange(rangemin, rangemax+1, Ls[0,0])\n",
    "    Ns_range_str = \", \".join(str(l) for l in Ns_range)\n",
    "    \n",
    "    # Label the fit\n",
    "    label = f\"Polyn. degree: {deg}\\n$N_s$ values: {Ns_range_str}\"\n",
    "    plt.plot(x_plot, y_plot, \"--\", label=label, alpha=0.7)\n",
    "    \n",
    "    \n",
    "x = 1/vol\n",
    "y = pseudo_bcs_r01\n",
    "plt.errorbar(x, y, yerr=yerr_r01, fmt=\"^\", label=r\"$\\beta_c ( N_s, N_t =$\"+f\"{Ls[0,0]}\"+r\"$)$\", capsize=7, color=\"blue\", markersize=7)\n",
    "plt.errorbar(0, median_val, yerr=[[error_lower], [error_upper]], fmt=\".\", label=r\"$\\beta_c (\\infty, N_t =$\"+f\"{Ls[0,0]}\"+r\"$)$ estimate\", capsize=7, color=\"red\", markersize=7)\n",
    "\n",
    "plt.xlabel(r\"$N_{s}^{-3}$\",fontsize=30)\n",
    "plt.ylabel(r\"$\\beta$\", fontsize=30,rotation=0,labelpad=15)\n",
    "plt.ylim([6.0596, 6.0700])\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.ticklabel_format(axis='x', style='sci', scilimits=(0, 0))\n",
    "ax.xaxis.get_offset_text().set_fontsize(20)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "# split handles/labels into \"fits\" vs \"others\"\n",
    "is_fit = [lab.startswith(\"Polyn.\") for lab in labels]\n",
    "fit_handles  = [h for h,f in zip(handles,is_fit) if f]\n",
    "fit_labels   = [l for l,f in zip(labels,is_fit) if f]\n",
    "other_handles = [h for h,f in zip(handles,is_fit) if not f]\n",
    "other_labels  = [l for l,f in zip(labels,is_fit) if not f]\n",
    "\n",
    "leg1 = ax.legend(\n",
    "    other_handles, other_labels,\n",
    "    loc='upper left',\n",
    "    frameon=True,\n",
    "    ncol=2,\n",
    "    fontsize=13\n",
    ")\n",
    "leg2 = ax.legend(\n",
    "    fit_handles, fit_labels,\n",
    "    loc='upper left',\n",
    "    bbox_to_anchor=(0.0, 0.85),\n",
    "    frameon=True,\n",
    "    title=\"Fits\",\n",
    "    ncol=1,\n",
    "    fontsize=12\n",
    ")\n",
    "ax.add_artist(leg1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"../reports/Nt={Ls[0,0]}/figures/fits_r01.pdf\",format=\"pdf\",bbox_inches='tight',pad_inches=0.025)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
